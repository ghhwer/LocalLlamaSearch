{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before you start:\n",
    "\n",
    "To run this code make sure you have the the libraries listed on requirements.txt on the same directory as this notebook.\n",
    "\n",
    "I'd recommend you to use a virtual environment to run this code. \n",
    "\n",
    "Also, Make sure the Docker container is running and the servers are bootstrapped.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import polars as pl\n",
    "from glob import glob\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "PARAGRAPH_SEP = \"\\n\\n\" # Paragraph separator\n",
    "\n",
    "GPU_SERVER_URL = 'http://localhost:11434'\n",
    "CPU_SERVER_URL = 'http://localhost:11435'\n",
    "\n",
    "EMBEDDING_MODEL = \"jina/jina-embeddings-v2-small-en\"\n",
    "MAIN_LLM = \"llama3\"\n",
    "\n",
    "DELTA_LOCATION = \"data/vector_store_delta\"\n",
    "CLEAN_DATA_FOLDER = True\n",
    "N_CTX = 8192\n",
    "\n",
    "# Define functions to interact with the models\n",
    "def get_embeddings(text):\n",
    "    response = requests.post(f'{CPU_SERVER_URL}/api/embeddings', json={\n",
    "        \"model\": EMBEDDING_MODEL,\n",
    "        \"prompt\": text\n",
    "    })\n",
    "    return response.json()['embedding']\n",
    "\n",
    "def get_model_response(history, temperature=0.1):\n",
    "    response = requests.post(f'{GPU_SERVER_URL}/api/chat', json={\n",
    "        \"model\": \"llama3\",\n",
    "        \"messages\": history,\n",
    "        \"options\":{\n",
    "            \"temperature\": temperature,\n",
    "            \"num_ctx\": N_CTX,\n",
    "        },\n",
    "        \"stream\": False\n",
    "    })\n",
    "    response_json = response.json()\n",
    "    llm_token_count = response_json.get('eval_count')\n",
    "    prompt_token_count = response_json.get('prompt_eval_count')\n",
    "    return response.json().get('message').get('content'), llm_token_count, prompt_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def approx_num_tokens_from_string(string: str) -> int:\n",
    "    return len(string.split(\" \"))\n",
    "\n",
    "def approx_sentense_split(text, chunk_target_size=1000, overlap=20, min_chunk_size=200, paragraph_sep='\\n\\n', chunking_regex='[^,\\.;]+[,\\.;]?', word_sep=' ') -> list[str]:\n",
    "    # Step 1: Split by paragraph separator\n",
    "    paragraphs = text.split(paragraph_sep)\n",
    "    chunks = []\n",
    "    \n",
    "    for paragraph in paragraphs:\n",
    "        # Step 2: Split by the second chunking regex\n",
    "        matches = re.findall(chunking_regex, paragraph)\n",
    "        \n",
    "        # Group the matches into chunks\n",
    "        current_chunk = []\n",
    "        current_chunk_size = 0\n",
    "        \n",
    "        for match in matches:\n",
    "            num_tokens = approx_num_tokens_from_string(match)\n",
    "            if current_chunk_size + num_tokens <= chunk_target_size:\n",
    "                current_chunk.append(match)\n",
    "                current_chunk_size += num_tokens\n",
    "            else:\n",
    "                # Finalize the current chunk if it exceeds the target size\n",
    "                if current_chunk:\n",
    "                    chunks.append(word_sep.join(current_chunk))\n",
    "                \n",
    "                # Start a new chunk with overlap from the end of the previous chunk\n",
    "                overlap_chunks = current_chunk[-overlap:] if len(current_chunk) >= overlap else current_chunk\n",
    "                current_chunk = overlap_chunks + [match]\n",
    "                current_chunk_size = sum(approx_num_tokens_from_string(c) for c in current_chunk)\n",
    "        \n",
    "        # Add the last chunk of the paragraph if not empty\n",
    "        if current_chunk:\n",
    "            chunks.append(word_sep.join(current_chunk))\n",
    "    \n",
    "    # Ensure no chunks are too small\n",
    "    merged_chunks = []\n",
    "    buffer_chunk = ''\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if approx_num_tokens_from_string(chunk) < min_chunk_size:\n",
    "            if buffer_chunk:\n",
    "                buffer_chunk += word_sep + chunk\n",
    "            else:\n",
    "                buffer_chunk = chunk\n",
    "            \n",
    "            if approx_num_tokens_from_string(buffer_chunk) >= min_chunk_size:\n",
    "                merged_chunks.append(buffer_chunk)\n",
    "                buffer_chunk = ''\n",
    "        else:\n",
    "            if buffer_chunk:\n",
    "                merged_chunks.append(buffer_chunk)\n",
    "                buffer_chunk = ''\n",
    "            merged_chunks.append(chunk)\n",
    "    \n",
    "    if buffer_chunk:\n",
    "        if merged_chunks:\n",
    "            merged_chunks[-1] += word_sep + buffer_chunk\n",
    "        else:\n",
    "            merged_chunks.append(buffer_chunk)\n",
    "    \n",
    "    return merged_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 1000\n",
    "\n",
    "def save_vector_to_table(vectors, delta_path):\n",
    "    print(f\"Appending {len(vectors)} rows to the delta table\")    \n",
    "    df = pl.DataFrame(vectors)\n",
    "    df.write_delta(delta_path, mode=\"append\")\n",
    "    \n",
    "def build_vector_dataframe(file_root_path, delta_path):\n",
    "    # Get the list of files\n",
    "    files = glob(f\"{file_root_path}/*.txt\")\n",
    "    # Process each file\n",
    "    for file in files:\n",
    "        print(f\"Processing file {file}\")\n",
    "        vector_data = []\n",
    "        with open(file, 'r') as f:\n",
    "            text = f.read()\n",
    "        # Clean the text a bit\n",
    "        text = re.sub(r\" +\", \" \", text) \n",
    "        text = re.sub(r\"\\n\\n+\", PARAGRAPH_SEP, text)\n",
    "        # Split the text into chunks\n",
    "        chunks = approx_sentense_split(text, CHUNK_SIZE)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Get the embeddings for the chunk\n",
    "            vector = get_embeddings(chunk) \n",
    "            # The vector fetch could by improved to be done in parallel and using batching\n",
    "            vector_data.append({'uri': file, 'text': chunk, 'vector': vector, 'chunk_id': i})\n",
    "        # Append the vector to the delta table\n",
    "        save_vector_to_table(vector_data, delta_path)\n",
    "\n",
    "# Check if data folder exists at the root\n",
    "data_folder = DELTA_LOCATION.split(\"/\")[0]\n",
    "if not os.path.exists(data_folder):\n",
    "    os.makedirs(data_folder)\n",
    "\n",
    "if not os.path.exists(DELTA_LOCATION):\n",
    "    # Build the delta table\n",
    "    build_vector_dataframe(\"files_examples\", DELTA_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    \"Cossine similarity between two vectors\"\n",
    "    dot_product = sum(a * b for a, b in zip(v1, v2))\n",
    "    magnitude = (sum(a ** 2 for a in v1) * sum(b ** 2 for b in v2)) ** 0.5\n",
    "    return dot_product / magnitude\n",
    "\n",
    "def change_orientation_dict(dict_data):\n",
    "    result = []\n",
    "    for i in range(len(list(dict_data.values())[0])):\n",
    "        result.append({key: dict_data[key][i] for key in dict_data.keys()})\n",
    "    return result\n",
    "\n",
    "def query_vector_emb(query, delta_location, top_k=2, n_neighboors=1, min_similarity=0.75):\n",
    "    df_emb = pl.scan_delta(delta_location)\n",
    "    query_embedding = get_embeddings(query)\n",
    "    df_q = (\n",
    "        df_emb\n",
    "            .with_columns(pl.lit(query_embedding).alias(\"query_embedding\"))\n",
    "            .with_columns(\n",
    "                pl.struct([\"query_embedding\", \"vector\"])\n",
    "                .map_elements(\n",
    "                    lambda cols: cosine_similarity(cols[\"query_embedding\"], cols[\"vector\"]),\n",
    "                    return_dtype=float\n",
    "                )\n",
    "                .alias(\"similarity\")\n",
    "            )\n",
    "            .filter(pl.col(\"similarity\") > min_similarity)\n",
    "    )\n",
    "\n",
    "    df_q_results = (\n",
    "        df_q.sort(\"similarity\", descending=True).select(\n",
    "            pl.col(\"uri\"),\n",
    "            pl.col(\"chunk_id\").alias(\"similar_chunk_id\"),\n",
    "            pl.col(\"similarity\").alias(\"similarity\"),\n",
    "        )\n",
    "        .limit(top_k)\n",
    "    )\n",
    "    \n",
    "    df_q_results = (\n",
    "        df_q_results\n",
    "        .join(\n",
    "            df_emb,\n",
    "            on=\"uri\"\n",
    "        ).filter(\n",
    "            (pl.col(\"chunk_id\") == pl.col(\"similar_chunk_id\")) |\n",
    "            (pl.col(\"chunk_id\") >= pl.col(\"similar_chunk_id\") - n_neighboors) &\n",
    "            (pl.col(\"chunk_id\") <= pl.col(\"similar_chunk_id\") + n_neighboors)\n",
    "        ).select(\n",
    "            pl.col(\"uri\"),\n",
    "            pl.col(\"text\"),\n",
    "            pl.col(\"similar_chunk_id\").alias(\"center_chunk_id\"),\n",
    "            pl.col(\"chunk_id\"),\n",
    "            pl.col(\"vector\"),\n",
    "            pl.col(\"similarity\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Retain the original scores, this can be used later for the UI\n",
    "    df_q_scores = df_q_results.select(\n",
    "        pl.col(\"uri\"),\n",
    "        pl.col(\"center_chunk_id\"),\n",
    "        pl.col(\"chunk_id\"),\n",
    "        pl.col(\"text\"),\n",
    "        pl.col(\"similarity\")\n",
    "    )\n",
    "\n",
    "    # Now group by the uri and get the text on the correct order\n",
    "    df_q_context = df_q_results.select(\n",
    "        pl.col(\"uri\"),\n",
    "        pl.col(\"chunk_id\"),\n",
    "        pl.col(\"text\")\n",
    "    ).unique().sort([\"uri\", \"chunk_id\"], descending=False)\n",
    "    df_q_context = df_q_context.group_by([\"uri\"], maintain_order=True).agg(\n",
    "        pl.col(\"text\").str.concat(\"\")\n",
    "    )\n",
    "    \n",
    "    # Convert to dictionary    \n",
    "    dict_context = df_q_context.collect().to_dict(as_series=False)\n",
    "    dict_detail = df_q_scores.collect().to_dict(as_series=False)\n",
    "    \n",
    "    return change_orientation_dict(dict_context), change_orientation_dict(dict_detail)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_keywords(query):\n",
    "  SYSTEM_PROMPT = f\"\"\"You are a helpful assistant that has access to pre-fetched query results.\n",
    "  Your job is to transform the user query into keywords separated by commas.\n",
    "\n",
    "  To ensure a good job follow the instructions:\n",
    "      1. Do not include any stopwords in the keywords.\n",
    "      2. Make sure to include the most important keywords first.\n",
    "      3. Do not explain the keywords, just list them.\n",
    "  \"\"\"\n",
    "  history = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": f\"Why is the sky blue?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"sky color explanation\"},\n",
    "    {\"role\": \"user\", \"content\": f\"How does GPU and CPU differ?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"GPU, CPU, difference\"},\n",
    "    {\"role\": \"user\", \"content\":query}\n",
    "  ]\n",
    "  text, _, _ = get_model_response(history, 0.2)\n",
    "  return [x.strip() for x in text.split(\",\")]\n",
    "\n",
    "def get_response(query, delta_location):\n",
    "    keywords = make_keywords(query)\n",
    "    emb_similarity, detail = query_vector_emb(', '.join(keywords), delta_location, n_neighboors=1)\n",
    "    \n",
    "    SYSTEM_PROMPT = \"\"\"You are an expert Q&A system that is trusted around the world.\n",
    "    Always answer the query using the provided context information, and not prior knowledge.\n",
    "    Some rules to follow:\n",
    "    1. Never directly reference the given context in your answer.\n",
    "    2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\n",
    "    3. When answering based on the context, always provide references from the context information.\n",
    "    4. Please format your answer in markdown.\"\"\"\n",
    "    USER_PROMPT = \"\"\"Context information is below.\n",
    "    ---------------------\n",
    "    [CONTEXT]\n",
    "    ---------------------\n",
    "    Given the context information and NOT prior knowledge, answer the query.\n",
    "    Query: [QUERY]\n",
    "    \"\"\"\n",
    "\n",
    "    # Fetch the context information\n",
    "    \n",
    "    final_context = \"\"\n",
    "    for sub_context in emb_similarity:\n",
    "        final_context += f\"URI: {sub_context['uri']}\\nText: {sub_context['text']}\\n\\n\"\n",
    "    USER_PROMPT = USER_PROMPT.replace(\"[CONTEXT]\", final_context)\n",
    "    USER_PROMPT = USER_PROMPT.replace(\"[QUERY]\", query)\n",
    "    history = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT},\n",
    "    ]\n",
    "    response, llm_token_count, prompt_token_count = get_model_response(history, 0.7)\n",
    "    return {\n",
    "      'llm_response':response,\n",
    "      'keywords_used': keywords, \n",
    "      'search_details':detail, \n",
    "      'llm_token_count':llm_token_count,\n",
    "      'prompt_token_count':prompt_token_count,\n",
    "      'warn': prompt_token_count > N_CTX\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_response(response):\n",
    "    print('--- LLM Response ---')\n",
    "    print(response.get('llm_response'))\n",
    "    print('--- SEARCH ENGINE SUMMARY ---')\n",
    "    keywords_used = response.get('keywords_used')\n",
    "    search_details = response.get('search_details')\n",
    "    chunks_used = len(search_details)\n",
    "    center_chunks_details = set(\n",
    "        [\n",
    "            f\"uri: {x['uri']}, center_chunk: {x['center_chunk_id']}, similarity: {x['similarity']}\" for x in search_details\n",
    "        ]\n",
    "    )\n",
    "    print('Keywords used:', keywords_used)\n",
    "    print('Chunks used:', chunks_used, 'over the context of:')\n",
    "    for detail in center_chunks_details:\n",
    "        print('\\t', detail)\n",
    "    print('--- LLM USAGE SUMMARY ---')\n",
    "    print('Prompt token count:', response.get('prompt_token_count'))\n",
    "    print('LLM token count:', response.get('llm_token_count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LLM Response ---\n",
      "**Apple Pasta Salad Recipe Ingredients**\n",
      "\n",
      "According to the provided context information, the ingredients for the **Apple Pasta Salad Recipe** are:\n",
      "\n",
      "1. 1 container (8 ounce) plain nonfat yogurt\n",
      "2. 2 cups uncooked rotini pasta\n",
      "3. 1 (8 ounce) can unsweetened crushed pineapple - undrained\n",
      "4. 1/2 cup shredded carrot\n",
      "5. 1 cup sliced celery\n",
      "6. 1/2 teaspoon salt - optional\n",
      "7. 1/4 cup sliced green onions\n",
      "8. 1/4 teaspoon garlic powder\n",
      "9. 1/4 cup raisins\n",
      "10. 1/4 teaspoon dry mustard\n",
      "11. 3 cups diced unpeeled York or Winesap apples\n",
      "12. 1 teaspoon finely chopped crystallized ginger\n",
      "13. 1 tablespoon honey\n",
      "\n",
      "These ingredients are listed in the context information as part of the **Apple Pasta Salad Recipe**.\n",
      "--- SEARCH ENGINE SUMMARY ---\n",
      "Keywords used: ['Apple pasta salad', 'ingredients', 'recipe']\n",
      "Chunks used: 6 over the context of:\n",
      "\t uri: files_examples\\Pasta.txt, center_chunk: 1, similarity: 0.8400889960039423\n",
      "\t uri: files_examples\\Pasta.txt, center_chunk: 6, similarity: 0.8423298481219478\n",
      "--- LLM USAGE SUMMARY ---\n",
      "Prompt token count: 2433\n",
      "LLM token count: 194\n"
     ]
    }
   ],
   "source": [
    "query = 'I want to make Apple Pasta Salad, what are the ingredients?'\n",
    "response = get_response(query, DELTA_LOCATION)\n",
    "print_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LLM Response ---\n",
      "**PPP Over Ethernet Discovery Process**\n",
      "=============================\n",
      "\n",
      "In the PPP Over Ethernet (PPPoE) protocol, two packets play a crucial role in initiating a PPP session: **PADI** (PPP Over Ethernet Active Discovery Initiation) and **PADO** (PPP Over Ethernet Active Discovery Offer).\n",
      "\n",
      "### PADI (PPP Over Ethernet Active Discovery Initiation)\n",
      "\n",
      "A Host sends a PADI packet to the broadcast address (`DESTINATION_ADDR` set to the broadcast address). The packet has:\n",
      "\n",
      "* `CODE` field set to 0x09\n",
      "* `SESSION_ID` set to 0x0000\n",
      "* Exactly one TAG of type `Service-Name`, indicating the service requested by the Host\n",
      "\n",
      "The PADI packet is used to initiate a PPP session, and it must contain exactly one TAG of type `Service-Name`.\n",
      "\n",
      "### PADO (PPP Over Ethernet Active Discovery Offer)\n",
      "\n",
      "When an Access Concentrator receives a PADI packet that it can serve, it replies with a PADO packet. The packet has:\n",
      "\n",
      "* `DESTINATION_ADDR` set to the unicast address of the Host that sent the PADI\n",
      "* `CODE` field set to 0x07\n",
      "* `SESSION_ID` set to 0x0000\n",
      "* Exactly one TAG of type `AC-Name`, containing the Access Concentrator's name\n",
      "* Any number of other TAGs of type `Service-Name`, indicating the services offered by the Access Concentrator\n",
      "\n",
      "The PADO packet is used to offer PPP services to the Host.\n",
      "\n",
      "### Differences and Similarities\n",
      "\n",
      "Key differences:\n",
      "\n",
      "* **Direction**: PADI is sent from the Host to the broadcast address, while PADO is sent from the Access Concentrator to the unicast address of the Host.\n",
      "* **Code**: The `CODE` field in a PADI packet is set to 0x09, while it's set to 0x07 in a PADO packet.\n",
      "\n",
      "Similarities:\n",
      "\n",
      "* Both packets are used for PPP session initiation and discovery.\n",
      "* Both packets contain TAGs with information about the service or services being offered or requested.\n",
      "* Both packets have a `SESSION_ID` field that must be set to 0x0000.\n",
      "--- SEARCH ENGINE SUMMARY ---\n",
      "Keywords used: ['PADI', 'PADO', 'difference', 'similarity']\n",
      "Chunks used: 6 over the context of:\n",
      "\t uri: files_examples\\rfc2516.txt, center_chunk: 8, similarity: 0.7691483712884883\n",
      "\t uri: files_examples\\rfc2516.txt, center_chunk: 7, similarity: 0.7773270954288681\n",
      "--- LLM USAGE SUMMARY ---\n",
      "Prompt token count: 1590\n",
      "LLM token count: 449\n"
     ]
    }
   ],
   "source": [
    "query = 'Explain what is PADI and PADO and their differences and similarities.'\n",
    "response = get_response(query, DELTA_LOCATION)\n",
    "print_response(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
